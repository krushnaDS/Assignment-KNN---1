{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ec6b76",
   "metadata": {},
   "source": [
    "##### Q1. What is KNN algorithm?\n",
    "\n",
    "KNN is one of the basic yet essential classification algorithm. It belongs to the supervised ML domain. KNN is a versatile and widely used due to its simplicity. It does not require any assumptions about the underlying data distribution. It can also handle both numerical and categorical data.It is also less sensitive for outliers compared to other algorithm.\n",
    "\n",
    "It works on a principal of similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2fbe2",
   "metadata": {},
   "source": [
    "##### Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "K value denotes the number of nearest neighborsto be considered. \n",
    "###### Cross-Validation:\n",
    "This is the most recommended method. It involves dividing your data into folds, training the model with different K values on each fold, and evaluating their performance on the remaining folds. The K value with the best average performance across folds is considered optimal.\n",
    "\n",
    "###### Experimentation:\n",
    "Ultimately, the best way to find the optimal K is to experiment with different values and observe their impact on your model's performance. This can involve plotting the error rate or accuracy for various K values and selecting the one that minimizes the error or maximizes the accuracy.\n",
    "\n",
    "###### Odd Values for K:\n",
    "It's recommended to choose an odd value for K to avoid ties in the voting process when determining the majority class during classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57a0a3",
   "metadata": {},
   "source": [
    "##### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "KNN Classifier:\n",
    "- Classifies data points into predefined categories.\n",
    "- Discrete value representing the class label.\n",
    "- Determines the most frequent class label among the k nearest neighbors of the new data points. This essentially impies a 'Majority vote' approach.\n",
    "\n",
    "KNN Regressor:\n",
    "- Predicts continous numerical values.\n",
    "- Continous numerical value.\n",
    "- Calculates the averageof the target values from the k nearest neighbors of the new data point.This essentially leverages the 'Local average' to estimate the value for the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89479648",
   "metadata": {},
   "source": [
    "##### Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Evaluating the performance of a K-nearest model depends on whether it's used for classification or regression.\n",
    "\n",
    "Classification:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Area under the ROC curve\n",
    "\n",
    "Regression:\n",
    "- Mean squared error\n",
    "- Root mean squared error\n",
    "- Mean absolute error\n",
    "- R-squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9db508",
   "metadata": {},
   "source": [
    "##### Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The 'Curse of dimensionality' refers to a various problems that arise when dealing with data with high dimensions/features. It describes a phenomenon where the performance of certain algorithms deteriorates as the number of dimensions or features increases.\n",
    "\n",
    "Key aspects arises due to curse of dimensionality\n",
    "\n",
    "- Sparse data\n",
    "- Increased computational capacity\n",
    "- Overfitting\n",
    "- Distance metrics lose effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0958c",
   "metadata": {},
   "source": [
    "##### Q6. How do you handle missing values in KNN?\n",
    "\n",
    "KNN algorithm can't directly handle missing values present in the data. Since they rely on comparing distance between data points for prediction, missing values would disrupt the calculation and lead to inaccurate results. here are common approaches to handle missing values before applying KNN:\n",
    "\n",
    "1. Removal :\n",
    "This involves simply removing data points with missing values. However this can be wasteful, especially if you have large dataset or the missing values are not random.\n",
    "\n",
    "2. Imputation :\n",
    "This involves filling in the missing values with estimated values. \n",
    "\n",
    "3. Encoding categorical features with missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a70c9",
   "metadata": {},
   "source": [
    "##### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "Similarities :\n",
    "\n",
    "- Simple and interpretable \n",
    "Both KNNs are relatively easy to understand and implement, making them good options for beginners or situations where interpretability is crucial.\n",
    "\n",
    "- No strong assumptions\n",
    "They don't require strong assumptions about the underlying data distributions making them adaptable to various problems.\n",
    "\n",
    "- Sensitive to outliers\n",
    "Both KNN can be sensitive to outliers in the data, which can significantly impact their performance.\n",
    "\n",
    "Differences :\n",
    "- Classifiers \n",
    "    - KNN classifier can perform well on simple classification tasks with well-defined class boundaries, especially when the data has high dimensionallity. However, their performance can deteriorate in complex datasets with noisy features or overlapping classes.\n",
    "    - KNN classifiers can be computationally expensive during prediction, especially for large datasets as they need to calculate distances to all data points to find the nearest neighbors.\n",
    "    \n",
    "- Regressors\n",
    "    - Knn regressors can be effective for local regression problems where the target variable is smooth and influenced by nearby data points.\n",
    "    - KNN regressor generally share the same computational drawback, but the impact might be slightly less severe compared due to simpler prediction steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5c1c9",
   "metadata": {},
   "source": [
    "##### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "###### Strengths :\n",
    "\n",
    "- Simplicity :KNN is straightforward to understand and implement, making it accessible even to those with limited machine learning experience.\n",
    "\n",
    "- Interpretability: Unlike some complex algorithms, KNN allows easy interpretation of results. By analyzing the nearest neighbors of a data point, we can understand the reasoning behind the prediction.\n",
    "\n",
    "- Non-parametric: KNN does not make assumptions about the underlying data distribution, making it suitable for various scenarios.\n",
    "\n",
    "- Effective for diverse data: KNN can work well with different data types, including numerical and categorical.\n",
    "\n",
    "##### Weaknesses:\n",
    "\n",
    "- Curse of dimensionality: As the number of features increases, the distance metric becomes unreliable, leading to inaccurate predictions. This is known as the \"curse of dimensionality.\"\n",
    "\n",
    "- Sensitivity to noise: Outliers and noisy data can significantly impact KNN's performance, potentially leading to misleading predictions.\n",
    "\n",
    "- Computationally expensive: During prediction, KNN needs to calculate distances to all data points in the training set, making it computationally expensive for large datasets.\n",
    "\n",
    "- High memory usage: KNN stores the entire training data, leading to high memory usage, especially for large datasets.\n",
    "\n",
    "##### Addressing the Weaknesses:\n",
    "Here are some approaches to address the limitations of KNN:\n",
    "\n",
    "- Dimensionality reduction techniques: Techniques like Principal Component Analysis (PCA) can be used to reduce the number of features, mitigating the \"curse of dimensionality.\"\n",
    "\n",
    "- Data pre-processing: Outlier detection and removal, along with noise reduction techniques, can improve the quality of data and enhance KNN's performance.\n",
    "\n",
    "- Approximation algorithms: Utilizing techniques like k-d trees or ball trees can significantly decrease the computational cost of finding nearest neighbors in high-dimensional spaces.\n",
    "\n",
    "- Sampling and incremental learning: Implementing techniques like sampling a subset of the data or using incremental learning algorithms can reduce memory usage and improve efficiency for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99066bc5",
   "metadata": {},
   "source": [
    "##### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "##### Euclidean Distance:\n",
    "\n",
    "- Represents the straight-line distance between two points in a multidimensional space.\n",
    "- Calculated using the square root of the sum of squared differences between corresponding coordinates of the two points.\n",
    "- Considers the magnitude of the differences along each dimension.\n",
    "- More susceptible to outliers due to its emphasis on larger differences.\n",
    "\n",
    "##### Manhattan Distance:\n",
    "\n",
    "- Represents the sum of the absolute differences between corresponding coordinates of two points.\n",
    "- Often referred to as the \"taxicab geometry\" distance, as it imagines traveling along a grid where you can only move horizontally or vertically.\n",
    "- Less sensitive to outliers compared to Euclidean distance, as it focuses on the absolute difference rather than the magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021d01c",
   "metadata": {},
   "source": [
    "##### Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling plays a importand role in KNN to ensure fair and accurate distance calculations,leading to better predictions.\n",
    "\n",
    "- Prevents Feature Domination:\n",
    "If feature are not scaled, the larger feature will dominate the distance calculation, bias the results towards that feature and neglect the influence of other features.\n",
    "\n",
    "- Ensures equal contribution:\n",
    "Feature scaling transforms features to a common scale, ensuring that each feature contributes equally to the distance calculation. this allows the algorithm to consider all features fairly when determining the nearest neighbors.\n",
    "\n",
    "- Improve model performance:\n",
    "By addressing feature dominance, scaling leads to more accurate distance calculations and consequently, better predictions and overall KNN performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db386766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
